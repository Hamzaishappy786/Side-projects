{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Installing libraries","metadata":{}},{"cell_type":"code","source":"# Fixed Cell 1\n# We install the new libraries but DO NOT force upgrade torch/torchvision\n# to avoid breaking the Kaggle environment.\n!pip install -q -U transformers peft datasets bitsandbytes trl accelerate wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:10:36.522071Z","iopub.execute_input":"2025-12-16T06:10:36.522746Z","iopub.status.idle":"2025-12-16T06:12:09.765287Z","shell.execute_reply.started":"2025-12-16T06:10:36.522712Z","shell.execute_reply":"2025-12-16T06:12:09.764349Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.4/517.4 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Conifuring secrets for huggingface","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    BitsAndBytesConfig,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTConfig, SFTTrainer\n\n# Disable WandB to prevent connection/protobuf errors\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:12:14.048229Z","iopub.execute_input":"2025-12-16T06:12:14.048627Z","iopub.status.idle":"2025-12-16T06:12:42.293539Z","shell.execute_reply.started":"2025-12-16T06:12:14.048598Z","shell.execute_reply":"2025-12-16T06:12:42.292943Z"}},"outputs":[{"name":"stderr","text":"2025-12-16 06:12:24.686639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765865544.871902      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765865544.928064      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":2},{"cell_type":"markdown","source":"### Weights configuration:","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:12:46.024477Z","iopub.execute_input":"2025-12-16T06:12:46.025539Z","iopub.status.idle":"2025-12-16T06:12:46.030437Z","shell.execute_reply.started":"2025-12-16T06:12:46.025509Z","shell.execute_reply":"2025-12-16T06:12:46.029656Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Loading pretrained model from kaggle","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\n# Disable caching for training and force config consistency\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:14:39.020256Z","iopub.execute_input":"2025-12-16T06:14:39.020923Z","iopub.status.idle":"2025-12-16T06:15:06.423389Z","shell.execute_reply.started":"2025-12-16T06:14:39.020895Z","shell.execute_reply":"2025-12-16T06:15:06.422824Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d119968a0c84a61869953ea375cce1a"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cc25c177f064c249bd712abf1e00eff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2c9e0442f124fa09d8b13483e128cbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddaba39a6773410694d62c4869b7971f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a2484ec97044d2e973ffd1e25291f0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e804ce6e6cf49589ec39632509e25f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45927575efc8456d8ee4aba7570417fa"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"### again loading, this time correctly","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# 1. Define Model Name\nMODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n\n# 2. Configure Quantization (Cell 3)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False,\n)\n\n# 3. Load Model (Cell 4)\nprint(\"Loading Model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nmodel.config.use_cache = False\n\n# 4. Load Tokenizer (Cell 5 - The missing piece!)\nprint(\"Loading Tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(\"Done! Now you can run Cell 6 (Dataset) and Cell 7/8 (Training).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:36:17.625495Z","iopub.execute_input":"2025-12-16T06:36:17.625789Z","iopub.status.idle":"2025-12-16T06:36:26.738623Z","shell.execute_reply.started":"2025-12-16T06:36:17.625750Z","shell.execute_reply":"2025-12-16T06:36:26.737990Z"}},"outputs":[{"name":"stdout","text":"Loading Model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9189e0e5a6a4834af9ef0698915ba4b"}},"metadata":{}},{"name":"stdout","text":"Loading Tokenizer...\nDone! Now you can run Cell 6 (Dataset) and Cell 7/8 (Training).\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### Loading dataset in urdu","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# 1. Load the working Urdu Alpaca dataset\nprint(\"Downloading real Urdu dataset...\")\ndataset = load_dataset(\"mahwizzzz/urdu_alpaca_yc_filtered\", split=\"train\")\n\n# 2. Inspect the raw data to see column names (Just to be sure!)\nprint(\"Raw columns:\", dataset.column_names)\n\n# 3. Format function (UPDATED to match the actual column names)\ndef format_instruction(sample):\n    # The columns are named 'urdu_instruction', 'urdu_input', 'urdu_output'\n    instruction = sample['urdu_instruction']\n    context = sample['urdu_input']\n    response = sample['urdu_output']\n    \n    # If there is extra context (input), add it to the user prompt\n    if context and len(str(context)) > 1:\n        user_prompt = f\"{instruction}\\nInput: {context}\"\n    else:\n        user_prompt = instruction\n        \n    # Create the final text string for the model\n    full_text = f\"User: {user_prompt}\\nAssistant: {response}\"\n    return {\"text\": full_text}\n\n# 4. Apply formatting to the whole dataset\nprint(\"Formatting dataset...\")\ndataset = dataset.map(format_instruction)\n\n# 5. Filter and Select\n# We select 500 examples.\ndataset = dataset.shuffle(seed=42).select(range(500))\n\nprint(f\"\\nFINAL DATASET READY: {len(dataset)} examples.\")\nprint(\"-\" * 50)\nprint(dataset[0]['text'])\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:36:28.041449Z","iopub.execute_input":"2025-12-16T06:36:28.042116Z","iopub.status.idle":"2025-12-16T06:36:28.889390Z","shell.execute_reply.started":"2025-12-16T06:36:28.042093Z","shell.execute_reply":"2025-12-16T06:36:28.888752Z"}},"outputs":[{"name":"stdout","text":"Downloading real Urdu dataset...\nRaw columns: ['urdu_instruction', 'urdu_input', 'urdu_output']\nFormatting dataset...\n\nFINAL DATASET READY: 500 examples.\n--------------------------------------------------\nUser: ایس کیو ایل ڈیٹا بیس کے لئے ایک سوال لکھیں۔\nInput: ایک مووی ڈیٹا بیس جس میں 'فلموں' کی ایک ٹیبل اور 'اداکاروں' کی ایک میز ہوتی ہے۔\nAssistant: فلموں کا عنوان منتخب کریں، actors.name\nفلموں سے\nاندرونی طور پر اداکاروں میں شامل ہو جائیں \nاو این movies.actor_id = actors.actor_id\nفلموں کے عنوان کے مطابق آرڈر؛\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### Preparing model for training, using PEFT","metadata":{}},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n)\n\nmodel = get_peft_model(model, peft_config)\n\n# *** STABILITY FIX ***\n# We force the trainable LoRA adapters to Float32. \n# This ensures gradients are calculated in a format the T4 GPU supports.\nfor name, param in model.named_parameters():\n    if \"lora\" in name:\n        param.data = param.data.to(torch.float32)\n\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:36:30.582718Z","iopub.execute_input":"2025-12-16T06:36:30.583247Z","iopub.status.idle":"2025-12-16T06:36:31.230043Z","shell.execute_reply.started":"2025-12-16T06:36:30.583224Z","shell.execute_reply":"2025-12-16T06:36:31.229421Z"}},"outputs":[{"name":"stdout","text":"trainable params: 29,933,568 || all params: 3,115,872,256 || trainable%: 0.9607\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### Training model on 1 epoch","metadata":{}},{"cell_type":"code","source":"sft_config = SFTConfig(\n    output_dir=\"./results\",\n    dataset_text_field=\"text\",\n    max_length=256,\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    learning_rate=2e-4,\n    logging_steps=10,\n    \n    # *** THE KEY SETTING ***\n    # strictly False to prevent the \"unscale_cuda\" crash\n    fp16=False,\n    bf16=False,\n    \n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"none\",\n    packing=False,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    processing_class=tokenizer,\n    args=sft_config,\n)\n\nprint(\"Starting training...\")\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:36:44.012256Z","iopub.execute_input":"2025-12-16T06:36:44.012542Z","iopub.status.idle":"2025-12-16T07:02:02.155487Z","shell.execute_reply.started":"2025-12-16T06:36:44.012522Z","shell.execute_reply":"2025-12-16T07:02:02.154920Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b57846743302458aa9c3005371cf3952"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6435f21852b44f1874353b778f3249d"}},"metadata":{}},{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 24:39, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.819200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.626000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.649200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.513100</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.524400</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.468100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=63, training_loss=1.598415177965921, metrics={'train_runtime': 1504.2836, 'train_samples_per_second': 0.332, 'train_steps_per_second': 0.042, 'total_flos': 2014408115945472.0, 'train_loss': 1.598415177965921, 'entropy': 1.5641355514526367, 'num_tokens': 116787.0, 'mean_token_accuracy': 0.6126447518666586, 'epoch': 1.0})"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"### Testing it out","metadata":{}},{"cell_type":"code","source":"def generate_response(prompt):\n    # Format the prompt using the chat template\n    messages = [\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n    # Tokenize and move to GPU\n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    # Generate response\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=100,      # Limit the response length\n        temperature=0.7,         # Creativity level\n        top_k=50,\n        top_p=0.9,\n        do_sample=True,          # Enable sampling for variety\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id\n    )\n\n    # Decode the output\n    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract just the assistant's reply (stripping the prompt)\n    # The split ensures we don't print the user's question twice\n    response = decoded_output.split(\"assistant\\n\")[-1].strip()\n    return response\n\n# --- Run a Test ---\ntest_prompt = \"اور سناو، تمہارا نام کیا ہے؟\"\nprint(f\"User: {test_prompt}\")\nprint(\"-\" * 50)\nprint(f\"Assistant: {generate_response(test_prompt)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:02:44.766388Z","iopub.execute_input":"2025-12-16T07:02:44.767146Z","iopub.status.idle":"2025-12-16T07:03:00.320036Z","shell.execute_reply.started":"2025-12-16T07:02:44.767118Z","shell.execute_reply":"2025-12-16T07:03:00.319429Z"}},"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\nCaching is incompatible with gradient checkpointing in Qwen2DecoderLayer. Setting `past_key_values=None`.\n","output_type":"stream"},{"name":"stdout","text":"User: اور سناو، تمہارا نام کیا ہے؟\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Assistant: م:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"### Saving model:","metadata":{}},{"cell_type":"code","source":"import shutil\n\n# 1. Save the model (Adapters) and Tokenizer locally\nnew_model_name = \"Qwen2.5-Urdu-Finetune\"\nprint(f\"Saving model to {new_model_name}...\")\n\nmodel.save_pretrained(new_model_name)\ntokenizer.save_pretrained(new_model_name)\n\nprint(\"Model saved successfully!\")\n\n# 2. (Optional) Zip the folder so you can download it easily\nprint(\"Zipping folder for download...\")\nshutil.make_archive(new_model_name, 'zip', new_model_name)\n\nprint(f\"Zip file created: {new_model_name}.zip\")\n\n# 3. Trigger Download (For Google Colab users)\n# If you are on Kaggle, the file will appear in the 'Output' section instead.\ntry:\n    files.download(f\"{new_model_name}.zip\")\nexcept:\n    print(\"Auto-download failed. Please download the ZIP file manually from the file explorer.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:03:20.154987Z","iopub.execute_input":"2025-12-16T07:03:20.155554Z","iopub.status.idle":"2025-12-16T07:03:27.223912Z","shell.execute_reply.started":"2025-12-16T07:03:20.155531Z","shell.execute_reply":"2025-12-16T07:03:27.223228Z"}},"outputs":[{"name":"stdout","text":"Saving model to Qwen2.5-Urdu-Finetune...\nModel saved successfully!\nZipping folder for download...\nZip file created: Qwen2.5-Urdu-Finetune.zip\nAuto-download failed. Please download the ZIP file manually from the file explorer.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"### For using it afterwards:","metadata":{}},{"cell_type":"code","source":"# Example of how to load it back later\nfrom peft import PeftModel\n\n# 1. Load Base\nbase_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\", ...)\n\n# 2. Load YOUR Adapters\nmodel = PeftModel.from_pretrained(base_model, \"Qwen2.5-Urdu-Finetune\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}