{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Install & Imports:","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets accelerate rouge_score\n\nimport torch\nimport pandas as pd\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"üî• Using: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T08:35:27.330204Z","iopub.execute_input":"2025-12-12T08:35:27.331002Z","iopub.status.idle":"2025-12-12T08:35:30.967050Z","shell.execute_reply.started":"2025-12-12T08:35:27.330974Z","shell.execute_reply":"2025-12-12T08:35:30.966048Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"üî• Using: cuda\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### Load & Prepare Data:","metadata":{}},{"cell_type":"code","source":"# 1. Load Dataset\nprint(\"‚è≥ Loading Grammarly CoEdIT dataset...\")\ndataset = load_dataset(\"grammarly/coedit\")\n\n# 2. Broader Filter (Accept Grammar & Style fixes)\n# We want anything that improves the text, not just \"formal\"\nprint(\"‚è≥ Filtering for relevant tasks...\")\nrelevant_dataset = dataset['train'].filter(\n    lambda x: any(keyword in x['src'].lower() for keyword in [\"formal\", \"grammar\", \"improve\", \"fix\", \"rewrite\"])\n)\n\n# 3. Remove \"Lazy\" Examples (Identity Mappings)\ndef is_significant_change(example):\n    # Skip if input and output are identical or too similar\n    if example['src'] == example['tgt']: return False\n    if abs(len(example['src']) - len(example['tgt'])) < 2: return False\n    return True\n\nprint(\"‚è≥ Cleaning lazy examples...\")\nfiltered_dataset = relevant_dataset.filter(is_significant_change)\n\ncount = len(filtered_dataset)\nprint(f\"‚úÖ Examples ready for training: {count}\")\n\n# 4. Smart Selection (Never crash again)\n# We take 5,000 examples OR the total count, whichever is smaller\nselect_count = min(count, 5000)\nsplit_data = filtered_dataset.shuffle(seed=42).select(range(select_count)).train_test_split(test_size=0.1)\n\nprint(\"\\n--- SAMPLE INPUT ---\")\nprint(split_data['train'][0]['src'])\nprint(\"\\n--- SAMPLE TARGET ---\")\nprint(split_data['train'][0]['tgt'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T08:36:32.980548Z","iopub.execute_input":"2025-12-12T08:36:32.981295Z","iopub.status.idle":"2025-12-12T08:36:34.522239Z","shell.execute_reply.started":"2025-12-12T08:36:32.981268Z","shell.execute_reply":"2025-12-12T08:36:34.521687Z"}},"outputs":[{"name":"stdout","text":"‚è≥ Loading Grammarly CoEdIT dataset...\n‚è≥ Filtering for relevant tasks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/69071 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9490fe3c70fd4a83b0463cbb5b88e25b"}},"metadata":{}},{"name":"stdout","text":"‚è≥ Cleaning lazy examples...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/30565 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ef5c093533c4533bc65426f6d8a25d2"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Examples ready for training: 30451\n\n--- SAMPLE INPUT ---\nFix grammar errors: But if you don't know the Potter very much, you might not understand it.\n\n--- SAMPLE TARGET ---\nBut if you don't know Harry Potter very much, you might not understand it.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### Tokenization:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# We use Flan-T5-Base because it's smarter at following instructions\nmodel_checkpoint = \"google/flan-t5-base\" \ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\ndef preprocess_function(examples):\n    # The 'src' column already has instructions (e.g., \"Fix grammar: ...\")\n    inputs = examples[\"src\"]\n    targets = examples[\"tgt\"]\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n    \n    # Tokenize targets\n    labels = tokenizer(text_target=targets, max_length=128, truncation=True)\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\nprint(\"‚è≥ Tokenizing dataset...\")\ntokenized_datasets = split_data.map(preprocess_function, batched=True)\nprint(\"‚úÖ Data ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T08:38:23.961275Z","iopub.execute_input":"2025-12-12T08:38:23.961605Z","iopub.status.idle":"2025-12-12T08:38:24.274710Z","shell.execute_reply.started":"2025-12-12T08:38:23.961581Z","shell.execute_reply":"2025-12-12T08:38:24.273868Z"}},"outputs":[{"name":"stdout","text":"‚è≥ Tokenizing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b10759972e1642d38371137f71ddb4c4"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Data ready.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### The Training Loop:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n\nargs = Seq2SeqTrainingArguments(\n    output_dir=\"/kaggle/working/professionalizer-flan\",\n    eval_strategy=\"epoch\",\n    learning_rate=3e-4,   # Slightly higher learning rate for Flan models\n    per_device_train_batch_size=8, # 8 is safe for T4 GPU\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=8,   # 8 epochs ensures it really learns the style\n    fp16=True,            # Faster training\n    report_to=\"none\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n    tokenizer=tokenizer,\n)\n\nprint(\"üöÄ Starting Training (Flan-T5-Base)...\")\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T08:38:33.169057Z","iopub.execute_input":"2025-12-12T08:38:33.169335Z","iopub.status.idle":"2025-12-12T09:00:53.108676Z","shell.execute_reply.started":"2025-12-12T08:38:33.169313Z","shell.execute_reply":"2025-12-12T09:00:53.107985Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90a40ada18df4b16a635d06a8b8cbfb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"178e2f7dbcf84b6a9d805b01d77b9b02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca83990d163143bbb128b67909e859d7"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_47/571129331.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"üöÄ Starting Training (Flan-T5-Base)...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2256' max='2256' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2256/2256 22:15, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.474482</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.484000</td>\n      <td>0.485988</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.484000</td>\n      <td>0.512646</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.257100</td>\n      <td>0.560032</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.257100</td>\n      <td>0.611605</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.154200</td>\n      <td>0.680361</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.154200</td>\n      <td>0.730010</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.099400</td>\n      <td>0.768420</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2256, training_loss=0.22940417891698525, metrics={'train_runtime': 1335.7987, 'train_samples_per_second': 26.95, 'train_steps_per_second': 1.689, 'total_flos': 3103528958300160.0, 'train_loss': 0.22940417891698525, 'epoch': 8.0})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"from transformers import pipeline\nimport shutil\n\n# 1. Test the model immediately\nprint(\"--- üß™ TESTING YOUR MODEL ---\")\nmy_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0)\n\ntest_inputs = [\n    \"Make this formal: yo bro where is the report?\",\n    \"Fix grammar: i dont like this code its bad.\",\n    \"Make this formal: i ain't doing that lol.\"\n]\n\nfor text in test_inputs:\n    result = my_pipeline(text, max_length=60)\n    print(f\"\\nInput:  {text}\")\n    print(f\"Output: {result[0]['generated_text']}\")\n\n# 2. Save & Zip for Download\nprint(\"\\n--- üì¶ ZIPPING FOR DOWNLOAD ---\")\nfinal_path = \"/kaggle/working/my_professionalizer\"\ntrainer.save_model(final_path)\ntokenizer.save_pretrained(final_path)\n\n# Create zip file\nshutil.make_archive(\"/kaggle/working/professionalizer_pack\", 'zip', final_path)\nprint(\"‚úÖ Done! Go to the 'Output' tab on the right sidebar to download 'professionalizer_pack.zip'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T09:00:56.103226Z","iopub.execute_input":"2025-12-12T09:00:56.103994Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nBoth `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"--- üß™ TESTING YOUR MODEL ---\n","output_type":"stream"},{"name":"stderr","text":"Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"\nInput:  Make this formal: yo bro where is the report?\nOutput: Where is the report?\n","output_type":"stream"},{"name":"stderr","text":"Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"\nInput:  Fix grammar: i dont like this code its bad.\nOutput: I don't like this code because it's bad.\n\nInput:  Make this formal: i ain't doing that lol.\nOutput: I'm not doing that, lol.\n\n--- üì¶ ZIPPING FOR DOWNLOAD ---\n","output_type":"stream"}],"execution_count":null}]}